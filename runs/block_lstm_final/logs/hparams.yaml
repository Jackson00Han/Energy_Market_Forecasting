activation: ReLU
dropout: 0.2
future_cov_dim: 0
hidden_dim: 256
input_chunk_length: 168
input_size: 72
likelihood: null
lr_scheduler_cls: !!python/name:torch.optim.lr_scheduler.CosineAnnealingLR ''
lr_scheduler_kwargs:
  T_max: 4
  eta_min: 1.0e-05
name: LSTM
nr_params: 1
num_layers: 1
num_layers_out_fc: []
optimizer_cls: !!python/name:torch.optim.adamw.AdamW ''
optimizer_kwargs:
  lr: 0.0006567984648551314
  weight_decay: 0.01
output_chunk_length: 24
output_chunk_shift: 0
target_size: 1
train_sample_shape:
- !!python/tuple
  - 168
  - 1
- !!python/tuple
  - 168
  - 71
- null
- null
- null
- !!python/tuple
  - 24
  - 1
use_reversible_instance_norm: false
