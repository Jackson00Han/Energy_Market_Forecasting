activation: ReLU
dropout: 0.4
future_cov_dim: 0
hidden_dim: 128
input_chunk_length: 720
input_size: 76
likelihood: null
lr_scheduler_cls: !!python/name:torch.optim.lr_scheduler.CosineAnnealingLR ''
lr_scheduler_kwargs:
  T_max: 200
  eta_min: 1.0e-05
name: LSTM
nr_params: 1
num_layers: 1
num_layers_out_fc: []
optimizer_cls: !!python/name:torch.optim.adamw.AdamW ''
optimizer_kwargs:
  lr: 0.00035576720056725817
  weight_decay: 0.01
output_chunk_length: 24
output_chunk_shift: 0
target_size: 1
train_sample_shape:
- !!python/tuple
  - 720
  - 1
- !!python/tuple
  - 720
  - 75
- null
- null
- null
- !!python/tuple
  - 24
  - 1
use_reversible_instance_norm: false
